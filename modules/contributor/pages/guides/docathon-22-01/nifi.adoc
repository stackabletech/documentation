= Overview

This section shows how to instantiate the first part of the entire processing chain, which will ingest CSV files from an S3 bucket, split the files into individual records and send these records to a Kafka topic.


== Installing Needed Operators

The definitions rolled out in this section need some operators to be installed in the K8s cluster.

=== Secret Operator

The secret operator is needed by the Stackable Operator for Apache NiFi, as NiFi requires the UI to be served via HTTPS.
The necessary certificates and keys for this are provided by the Secret Operator to the NiFi pods.

[source,bash]
helm repo add stackable-stable https://repo.stackable.tech/repository/helm-stable/
helm install secret-operator stackable-stable/secret-operator

=== ZooKeeper Operator

Apache NiFi and Apache Kafka both use Apache ZooKeeper as backing config storage, so the Stackable Operator for Apache ZooKeeper has to be installed in order to make sure that a ZooKeeper cluster can be rolled out.
There is no need to install multiple ZooKeeper clusters, as NiFi, Kafka and Druid can share the same cluster via provisioning a zNode per backed service.

[source,bash]
helm install zookeeper-operator stackable-stable/zookeeper-operator

=== Kafka Operator

Kafka will be used by NiFi to publish the individual records from the S3 data into.

[source,bash]
helm install kafka-operator stackable-stable/kafka-operator

=== NiFi Operator

NiFi is an ETL tool which will be used to model the dataflow of downloading and splitting files from S3.
It will also be used to convert the file content from CSV to JSON.

[source,bash]
helm install nifi-operator stackable-test/nifi-operator --version=0.6.0-pr251

NOTE: We need the PR version which has increased PVC sizes and heap memory.
Apply the Kafka custom resource (https://github.com/stackabletech/kafka-operator/blob/main/examples/simple-kafka-cluster.yaml)

== Installing Tools

For the purposes of this use case, no special config is required, so we can simply deploy the examples straight from the operator repositories.
These files are designed to be self-contained, so for example the NiFi example also contains the necessary ZooKeeper cluster definition.

[source,bash]
kubectl apply -f https://raw.githubusercontent.com/stackabletech/kafka-operator/main/examples/simple-kafka-cluster.yaml
kubectl apply -f https://raw.githubusercontent.com/stackabletech/nifi-operator/main/examples/simple-nifi-cluster.yaml

The Nifi installation will take several minutes due to the size of the images.

== Connecting to NiFi and Deploying the Flow

After all pods are succesfully deployed the NiFi UI should be accessible.
To retrieve the appropriate URL you can run the following command:

[source,bash]
kubectl get svc simple-nifi -o json | jq -r --argfile endpoints <(kubectl get endpoints simple-nifi -o json) --argfile nodes <(kubectl get nodes -o json) '($nodes.items[] | select(.metadata.name == $endpoints.subsets[].addresses[].nodeName) | .status.addresses | map(select(.type == "ExternalIP" or .type == "InternalIP")) | min_by(.type) | .address | tostring) + ":" + (.spec.ports[] | select(.name == "https") | .nodePort | tostring)'

This will output all UI endpoints for the NiFi cluster, the only thing you need to do is prepend 'https://' when accessing the UI in your browser. If your browser warns you that the connection is not secure (because of a self-signed certificate) you must continue to the unsafe variant.

image::docathon-2022-01/nifi-login.png[NiFi Login Screen]

The login credentials are defined in the https://github.com/stackabletech/nifi-operator/blob/main/examples/simple-nifi-cluster.yaml#L33[NiFi example].
Unless you changed these before deploying the cluster you will be able to log in with _admin / supersecretpassword_.


Once you have successfully logged in you should be presented with the NiFi UI showing an empty canvas.
This canvas is the main place where you will interact with NiFi. You can drag processors on here, configure them as needed and connect these processors to create a flow that offers the processing that you need.

NOTE: As this guide is not intended to be a NiFi guide most of NiFi's features will be glossed over and only very brief instructions provided on what needs to be done to get the flow up and running.

Some keywords will be linked to the NiFi documentation if you are interested in doing some more reading on the topic.

For the purpose of this guide we have prepared a flow which you can simply upload to NiFi as a template.
You can download this template modules/contributor/downloads/s3-kafka.xml[here]

To then upload this you need to click on the _upload template_ button in the UI and specify the appropriate file.

image::docathon-2022-01/nifi-uploadtemplate.png[Upload template to NiFi]

To deploy the template as a flow you need to click on the _template_ button in NiFis main menu.

image::docathon-2022-01/nifi-createtemplate.png[Create flow from template]

After you have done this, you should be presented with a simple flow on your canvas that is almost ready to start processing data.
The only thing that still needs doing is to enable some https://nifi.apache.org/docs.html[ControllerServices] used by the processors.

To get to these services you can right click on the https://nifi.apache.org/docs.html[SplitRecord] processor, go to the _properties_ tab and click on one of the small arrows next to the _Record Reader_ and _Record Writer_ options.

image::docathon-2022-01/nifi-controllerservices.png[Configure controller services]

On the controller page you then need to enable all three services by clicking on the small lightning symbol next to every service.
You will be presented with a confirmation dialog but no further action should be needed here.

image::docathon-2022-01/nifi-enablecontroller.png[Enable controller services]

Once this is done return to the main canvas and you are ready to start your flow and get data going.
To start the entire flow make sure that you don't have any processors selected by simply clicking on the emtpy canvas anywhere.
If you click the start button now, NiFi will start all processors and data should start flowing through and end up in the pre-configured Kafka topic.

NOTE: The flow in its packaged form has been restricted to only download a small subset of the yellow cab dataset, as the full size data is fairly large.
If you have the capacity to process all data you can remove this restriction in the _prefix_ property of the https://nifi.apache.org/docs.html[ListS3] processor, as shown in the screenshot below.

image::docathon-2022-01/nifi-prefix.png[Download filter]

If you change the highlighted value to `trip data/yellow_tripdata_` all data for yellow cabs will be downloaded.

You can now head over to the link:druid.adoc#_overview[Druid] section of this guide and continue with setup there.









