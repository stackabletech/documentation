= Overview

This section describes how to set up a streaming job in Druid to ingest data from the Kafka topic that was used with Nifi. Druid requires a backing storage (so called Deep-Storage) where data - partioned by date or time - is persisted as immutable "segments". Druid can use either local storage (only appropriate for stand-alone testing - i.e. all druid components run on the same machine), S3 or HDFS. In this guide we will use S3, specifically MinIO which is an S3-implementation suitable for low-footprint scenarios.

== Deploy MinIO

[source,bash]
helm install minio --set resources.requests.memory=8Gi --set mode=standalone --set replicas=1  --set persistence.enabled=false  --set "buckets[0].name=nytaxidata,buckets[0].policy=none" --set "users[0].accessKey=minioAccessKey,users[0].secretKey=minioSecretKey,users[0].policy=readwrite" --repo https://charts.min.io/ minio

N.B.

- we are specifying a memory allocation of 8GB as Min-IO will use 16GB by default.
- the access credentials are given above in the form of a secret, which will be used later in the Druid custom resource.

== Deploy Postgresql (for Druid metadata)

Druid requires a database to store metadata: for this guide we will use the Bitnami PostgreSQL helm chart to deploy a PostgreSQL instance:

[source,bash]
helm install postgresql-druid \
    --repo https://charts.bitnami.com/bitnami postgresql \
    --set auth.username=druid \
    --set auth.password=druid \
    --set auth.database=druid \
    --version 11.0.0

N.B. the credentials will also be used in the Druid custom resource.

== Deploy the Stackable Druid operator

[source,bash]
helm install druid-operator stackable-stable/druid-operator

N.B. make sure this is not executed in a location that contains a folder called "druid-operator" as the helm installation will not complete successfully!

== Deploy the Druid secret

[source]
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: druid-s3-credentials # <1>
stringData:
  accessKeyId: minioAccessKey
  secretAccessKey: minioSecretKey
EOF

== Deploy the Druid ZNode

This is required to link our Druid installation to the Zookeeper that we installed earlier:

[source]
cat <<EOF | kubectl apply -f -
apiVersion: zookeeper.stackable.tech/v1alpha1
kind: ZookeeperZnode
metadata:
  name: simple-druid-znode # <2>
spec:
  clusterRef:
    name: simple-zk
EOF

== Deploy the Druid cluster

[source]
cat <<EOF | kubectl apply -f -
apiVersion: druid.stackable.tech/v1alpha1
kind: DruidCluster
metadata:
  name: druid-nytaxidata
spec:
  version: 0.22.1
  zookeeperConfigMapName: simple-druid-znode  # <2>
  metadataStorageDatabase:
    dbType: postgresql
    connString: jdbc:postgresql://postgresql-druid/druid
    host: postgresql-druid
    port: 5432
    user: druid
    password: druid
  s3:
    endpoint: http://minio:9000
    credentialsSecret: druid-s3-credentials  # <1>
  deepStorage:
    storageType: s3
    bucket: nytaxidata
    baseKey: storage
  brokers:
    configOverrides:
      runtime.properties:
        druid.s3.enablePathStyleAccess: "true"
    roleGroups:
      default:
        selector:
          matchLabels:
            kubernetes.io/os: linux
        config: {}
        replicas: 1
  coordinators:
    configOverrides:
      runtime.properties:
        druid.s3.enablePathStyleAccess: "true"
    roleGroups:
      default:
        selector:
          matchLabels:
            kubernetes.io/os: linux
        config: {}
        replicas: 1
  historicals:
    configOverrides:
      runtime.properties:
        druid.s3.enablePathStyleAccess: "true"
    roleGroups:
      default:
        selector:
          matchLabels:
            kubernetes.io/os: linux
        config: {}
        replicas: 1
  middleManagers:
    configOverrides:
      runtime.properties:
        druid.s3.enablePathStyleAccess: "true"
    roleGroups:
      default:
        selector:
          matchLabels:
            kubernetes.io/os: linux
        config: {}
        replicas: 1
  routers:
    configOverrides:
      runtime.properties:
        druid.s3.enablePathStyleAccess: "true"
    roleGroups:
      default:
        selector:
          matchLabels:
            kubernetes.io/os: linux
        config: {}
        replicas: 1
EOF

<1> S3 secret
<2> Druid ZNode

== Data Ingestion

There are different ways to get data into Druid, all of which will use a `POST` of a Druid-compatible ingestion specification. We will document here two ways of doing this, either directly in the Druid UI, or - this is e.g. useful if the job is to be repeated - by extracting the ingestion specification into a JSON file and issuing a curl from the command line (some of what follows is also covered in more depth in the official Druid documentation, but is mentioned here for the sake of completion).

=== Using the Druid UI

==== Setup port-forwarding for the Druid UI

Run this from the command line to open up access to the Druid router (keep this command line tab open):

[source,bash]
kubectl port-forward svc/druid-nytaxidata-router 8888

==== Druid UI

The UI should now be reachable at http://localhost:8888 and should look like the screenshot below. We will start with the “Load Data” option:

image::docathon-2022-01/druid-main.png[Main Screen]

Select "Apache Kafka" and then "Connect Data" at the right of the screen, entering the following in the two available fields:

- Bootstrap servers: `simple-kafka:9092`
- Topic: `nytaxidata`

Then select "Start of stream" and then "Apply":

image::docathon-2022-01/druid-connect.png[Connect to Kafka]

At the bottom right of the screen click through

- “Parse Data”, “Parse Time”, “Transform”, “Filter”, “Configure Schema”

without changing anything. At the next step - “Partition” - select `day` for the granularity:

image::docathon-2022-01/druid-partition.png[Partition]

Then click on “Tune”. At this point we tell Druid how to manage the Kafka offsets. As this is the initial read action we have to choose “True” so that Kafka starts at the earliest possible offset (subsequent reads will pick up from the last offset that Druid has cached internally):

image::docathon-2022-01/druid-tuning.png[Offsets]

Click through “Publish” to show “Edit spec”. At this point we have a complete ingestion job specification in JSON format:

image::docathon-2022-01/druid-jobspec.png[Ingestion-spec]

At this point we can just click on the final step on the bottom (“Submit”) and the job will start running - since the job is a streaming job it will wait for fresh Kafka data in the specified topic and ingest it into Druid. However, before we do that, save the JSON specification in a separate file (e.g. `/tmp/kafka-ingestion-spec.json`) as we will also show how to start this job from the command line per `curl`.

Back at the screen, click on “Submit” - the ingestion job will be started, which will take a few moments. As mentioned already, we are starting a streaming job, so it will continue to run in the background (i.e. the status remains `RUNNING`):

image::docathon-2022-01/druid-task.png[Task]

The magnifying glass icon shows metadata such as logs, spec-definition etc:

image::docathon-2022-01/druid-running.png[Running job]

Once the ingestion job has been started, Druid monitors the relevant Kafka topic for changes and ingest new data, persisting it in its deep storage. It can take a few moments for the first segments to be ready (and a bit longer until they are published as immutable segments in deep storage). The streaming job will stay at RUNNING until such time as it is stopped. The datasource is visible under the “Datasources” tab, where the individual segments - partitioned by time slice - can also be examined:

image::docathon-2022-01/druid-datasources.png[Datasources]

We can also display data by issuing queries against our datasource from within the SQL designer under the “Query” tab:

image::docathon-2022-01/druid-query.png[Query screen]

=== Using `curl`

We will now perform the same action using the JSON specification we saved earlier (in this guide: `/tmp/kafka-ingestion-spec.json`).

==== Setup port-forwarding for Druid Co-ordinator

Issue a port-forwarding command so that we can access the Druid co-ordinator from outside the the Kubernetes cluster:

[source]
kubectl port-forward svc/druid-nytaxidata-coordinator 8081

==== Post Job Specification

Issue a POST via curl, referencing the JSON specification:

[source]
curl -X POST -H 'Content-Type: application/json' -d @/tmp/kafka-ingestion-spec.json http://localhost:8081/druid/indexer/v1/supervisor

This should yield a status code of 200 with a response of `{"id":"nytaxidata"}`.

N.B. We have extracted our ingestion specification from the UI, where the datasource was created as part of the process, but we could also run this job without an existing datasource, as the job will create it if needed.

