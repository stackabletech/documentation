= Setting up an End-to-End Data Pipeline
:toc: macro

In this tutorial we will look at setting up a data pipeline, from raw data to visualization. We will read data from S3 using NiFi, send it to Kafka, from there it is ingested into Druid and lastly we will visualize the data using Superset.

toc::[]

== About this Tutorial
This is a tutorial intended for learning more about the Stackable platform, it is not a guide to building a robust data pipeline.

This is intended for use in a private network or lab; it doesn’t enable many security features such as authentication or encryption and should not be directly connected to the Internet. Be careful if you’re deploying in the cloud as your instances may default to using public IPs.

== General Setup

Before we get started, you should make sure that you have everything you need:

* A running Kubernetes cluster
* Kubectl
* Helm
* Shell utilities like `cat` and `curl`

Throughout the tutorial we will install Operators from our Stackable Helm repository using Helm. To easily install from the Stackable repository, add a link to it to your Helm repository list:

[source,bash]
helm repo add stackable-stable https://repo.stackable.tech/repository/helm-stable/

include::nifi.adoc[]

include::druid.adoc[]

include::superset.adoc[]