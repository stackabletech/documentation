= How to provide out-of-cluster access for products with built-in routing
Doc Writer <doc.writer@asciidoctor.org>
v0.1, YYYY-MM-DD
:status: draft

* Status: {status}
* Deciders: [list everyone involved in the decision] <!-- optional -->
* Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->

Technical Story: [description | ticket/issue URL] <!-- optional -->

== Context and Problem Statement
// Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.

Some products like HDFS and Kafka don't use a single router or portal node to access the cluster, but have the client access multiple nodes. For example, HDFS name nodes will tell the client where data can be found (hostname/IP and port), the client is then expected to connect directly to a specific data node. Similarly for Kafka and topic shards.

Problem:

* In our current setup, there's no way to connect to a specific replica in a StatefulSet or Deployement.
* The hostname and IP that the pods know about themselves is from _inside_ the cluster. The IP only works inside the overlay network.
* Clients might cache data locations that the received, and Kubernetes might move pods around. For some routing solutions, this can be a problem. (Stickiness)

== Decision Drivers
// Which criteria are useful to evaluate solutions?

* At least for HDFS, connections to individual pods will be used to transmit data, this means that performance is relevant.
* On-prem customers will often not have any kind of network-level load balancing (at least not one that is configurable by K8s)
* Cloud customers will often have relatively short-lived K8s nodes
* The solution should be minimally invasive - no large setups required outside of the cluster

== Implemented Solution

A solution was implemented.

There is a new lb-operator (name still not final) which runs as a DaemonSet and uses a CSI plugin to provide pseudo storage with files in it with information about the outside connection of the Pod.

Communication flow example using the HDFS Operator:
- The HDFS Operator requests a PVC of the lb.stackable.tech type. In the annotations it can request the lb class (`nodeport`, `loadbalancer`), for this example it'll be the `nodeport` type.
- The lb operator provisions a NodePort Service for the volume request, which means a Service per Pod. It reads the NodePort IP and port.
- The lb operator provisions the volumes with files inside containing information about the pods outside address and port - The IP and port of the NodePort Service. Because of the PVC it knows which pod the volume will be mounted into, and can find out the NodePort that belongs to the pod.
- the operator already provisioned the pod with a script that read the files from the mounted volume into environment variables which are then read by HDFS. This part is product specific.

The way the operator requests the volume is identical for all pods: it always requests a volume with the same type (i.e. `nodeport`).

== Decision Outcome

There is only one design, which is already in its implementation.


Pros:

* There's no little to know routing overhead (compared to proxying or similar).
* The lb operator can be extended to support more LoadBalancerClasses.
* It is a very low-friction solution that doesn't require a lot of permissions to set up.

Cons:

* Products like HDFS and Kafka only support having a single address. This means that if outside access with the lb operator is configured, all traffic will be routed that way.
* It is another DaemonSet Operator, which means more stuff that is running. It is also not clear how we will get this certified with OpenShift.

// TODO write positive and negative consequences

== Other notes

=== Spiked Alternative: MetalLB
See: https://metallb.universe.tf/

SpikeLB is a bare metal load balancer that was spiked briefly. However it requires BGP integration, which is not feasible as a requirement for customer installations.
