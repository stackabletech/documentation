= ADR024: How to provide stable out-of-cluster access to products
Felix Hennig <felix.hennig@stackable.tech>
v0.1, 2022-09-06
:status: accepted

* Status: {status}
* Deciders:
** Teo Klestrup RÃ¶ijezon
** Felix Hennig
** Vladislav Supalov
* Date: 2022-09-06

Technical Story: https://github.com/stackabletech/listener-operator/pull/1

== Context and Problem Statement
// Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.

Eventually, the products we host in Kubernetes will need to be accessed from outside of the cluster, as this is where the client is. Our current solution for this is NodePort services. They are a simple and common solution for on-premise clusters, where nodes are reachable hosts in the local network. To get traffic into a Kubernetes cluster that runs in a public cloud, NodePorts do not work; instead LoadBalancers are the preferred solution.

But both NodePorts and LoadBalancers have their individual problems for certain applications.
While a Pods name is stable across restarts and rescheduling, the IP and port of the NodePort can change if a Pod is rescheduled to a different node. This means that external addresses from simple NodePorts are not stable.
If a LoadBalancer is used to connect to a StatefulSet, the individual replicas in the set are not individually addressable anymore as they are with NodePorts. Some products need to be able to link to _specific_ replicas in a StatefulSet, as they shard data across process instances, across nodes. Therefore the nodes need to also be individually reachable from outside of the cluster.

Additionally, Pods currently do not know the address under which they are reachable from outside of the cluster, no matter if NodePorts or LoadBalancers are used. While this is not a problem for simple web UIs, it is a problem for products that do their own "routing", like HDFS or Kafka. These products will link to other nodes to point clients to specific data that only exists in specific nodes. These links cannot be constructed if the addresses under which nodes are reachable are not known to the product.


Problems:

* **Unstable addresses** - Clients need stable addresses to connect to, but Kubernetes can move pods around. While the discovery ConfigMap is updated, it's not feasible to ask the client to pull the new info from there every time, clients will want to use static config files with static addresses to connect to.
* **Replicas not addressable** - In our current setup, there's no way to connect to a specific replica in a StatefulSet or Deployement - which is necessary for cases like the data nodes of HDFS.
* **Pods don't know their outside address** - The hostname and IP that the pods know about themselves is from _inside_ the cluster. The IP only works inside the overlay network. This means ProductCluster processes cannot link to other nodes of the cluster.

== Decision Drivers
// Which criteria are useful to evaluate solutions?

* At least for HDFS, connections to individual pods will be used to transmit data, this means that performance is relevant.
* On-prem customers will often not have any kind of network-level load balancing (at least not one that is configurable by K8s).
* Cloud customers will often have relatively short-lived K8s nodes.
* The solution should be minimally invasive - no large setups required outside of the cluster.

== Considered Options

Off-the-shelf solutions were briefly spiked, but discarded due to requiring complex setup and/or heavy out-of-cluster dependencies which were deemed unacceptable to require from customers. Brief notes on this can be found in <<_spiked_alternatives>> below.

The other alternative is a custom solution to be implemented by us. It is outlined below.

== Implemented Solution

A new resource is proposed: Listener. It is handled similarly to storage. There is are ListenerClasses for different types of Listeners - analogous to StorageClass. There are Listener objects - similar to PersistentVolumes. And claims to listeners are made in ProductCluster objects.

ListenerClasses allow for various different ways of getting outside traffic into the cluster. A dedicated operator seperates the deployment of this out - the product operators need to only request the listeners.

Under the hood a listener-operator runs as a CSI driver with a new `listener.stackable.tech` type. Listener claims in the ProductCluster resource are then converted by the product operator into PersistentVolumeClaims (PVCs) to the storage type. Listener settings are passed along as annotations to the PVC. Initially there will be two Listener types - `private` implemented with NodePorts; and `public` implemented with LoadBalancers. The listener-operator creates the Listeners according to the PVC settings and provides the listener info in the PV into the pods with the PVCs.

Communication flow example using the HDFS Operator:

* A HDFS cluster resource is created by the user, with a `private` listener setting.
* The HDFS Operator requests a PVC of the listener.stackable.tech type and an annotation to create a `private` listener.
* The listener-operator provisions a NodePort Service for the volume request, which means a Service per Pod. It reads the NodePort IP and port.
* The listener-operator provisions the volumes with files inside containing information about the pods outside address and port - The IP and port of the NodePort Service. Because of the PVC it knows which pod the volume will be mounted into, and can find out the NodePort that belongs to the pod.
* the HDFS operator already provisioned the pod with a script that read the files from the mounted volume into environment variables which are then read by HDFS. This part is product specific.

// Setup is done, how does the client connect now?
The NodePorts are now created and HDFS knows about them, the client can now connect to HDFS from outside the cluster. An initial connection to HDFS is made through a NodePort, the address and port are found in the HDFS xref:concepts:service_discovery.adoc[]. Through the mechanism described above, any addresses of other nodes that HDFS gives to the client will be NodePort addresses, so subsequent connections will go through the NodePorts too.

How are the problems in the <<_context_and_problem_statement,Problem Statement>> addressed?

* **Unstable Addresses** - Using a CSI driver and mounting in storage automatically creates stickiness. Any new pods after a pod crashes will be created on the same node as the old pod - and thus also reuse the NodePort and the address it has.
* **Replicas not addressable** - Since every replica in a StatefulSet will have its own Listener, they are also individually addressable
* **Pods don't know their outside address** - The outside address of a pod gets passed into the pod through the mounted volume. The pod then knows its outside address.

=== How the name came to be

The new operator handles resources related to bringing outside traffic into the cluster. Some words that come to mind were Ingress and Gateway, but they are already used by Kubernetes native objects. Initially LoadBalancer-operator was considered, but since it doesn't exclusively deploy LoadBalancer objects (also NodePorts), the name is not good.

Listener describes well its functionality: It is listening for outside traffic. Also, the name is not taken in Kubernetes yet.

== Decision Outcome

There is only one design, which is already in its implementation.


Pros:

* There is little routing overhead (compared to proxying or similar).
* The listener-operator can be extended to support more ListenerClasses.
* It is a very low-friction solution that doesn't require a lot of permissions to set up.

Cons:

* The processes of some products like HDFS and Kafka assume that they are only reachable under one specific address. They cannot, for example, use one network for internal communication and a different network for external communication. This means that if outside access with the listener operator is configured, all traffic will be routed that way, also internal traffic that would not need to be routed out of the cluster.
* This operator is deployed as a `DaemonSet` , which means it adds a small amount of overhead on all nodes inside the cluster and to the control plane's api server.
* Such an operator cannot be deployed using OpenShift's Operator Lifecycle Manager and consequently cannot be certified on that platform.

== Spiked Alternatives

Some notes about the briefly tested off-the-shelf solutions.

=== MetalLB
link:https://metallb.universe.tf/[MetalLB] is a bare metal load balancer that was spiked briefly. However it requires BGP/ARP integration, which is not feasible as a requirement for customer installations.

With ARP, the LoadBalancers appear as "real" IP addresses in the same subnet as the nodes (with no need to configure custom routing roules). However, this scales poorly (it assumes that all nodes are in the same L2 broadcast domain) and is relatively likely to be blocked by firewalls or network policy.

=== Calico

link:https://www.tigera.io/project-calico/[Calico] requires BGP, another component that we cannot make required for customer setups.