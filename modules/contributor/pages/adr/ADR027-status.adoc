= [Resource Status]
Sebastian Bernauer <sebastian.bernauer@stackable.tech>
v0.1, 2023-02-28
:status: draft

* Status: {status}
* Deciders:
** Malte Sander
** Razvan Mihai
* Date: 2023-02-28

Technical Story: https://github.com/stackabletech/issues/issues/343

== Context and Problem Statement

// Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.

Resources managed by operators must offer a consistent and easy interface to query their availabilty. This is important for users to determine if the resource is functioning properly as well as for automated processes to ensure that flows or pipelines reach certain states and can continue.

Possible predefined conditions:
* Available (OpenShift)
* Progressing (OpenShift)
* Degraded (OpenShift)
* Upgradable (OpenShift)
* Paused (Stackable)
* Stopped (Stackable)

== Decision Drivers

The status field should be set and maintained for cluster resources as well as sub-resources (ex. `ZNode`, `SupersetDB`, etc.).

Must haves for cluster resources and sub-resources:
- Status string field to allow easy programatic query of the resource state (Available, Degraded, Stopped, Paused, ...).
- Status conditions as a generic and easy way to check the availability of the cluster and transition history.

Must have for cluster resources:
- Deployed product version. Used for in-place product upgrades/downgrades/migrations.
  
Nice to have based on product functionality:
- task/qurey load status (ram, cpu)
- disk pressure
- security status (tls on/off)
- accessability (external, internal k8s cluster only, hybrid)


== Considered Options

=== Set Status Conditions

The status conditions can be set in a generic way by querying the sub-resources of a cluster resource and applying logical opertors to compute the cluster status. For example, the Superset operator, queries the status of all StatefulSets, DB jobs, etc. and computes the cluster conditions based on all of them. 

If all sub-resources have a corresponding "Available" type with status `True` then, the custer will also have a condition of type  "Available" and status `True`. If any of the sub-resources are not `Available`, then the cluster status will reflect this by setting the condition `Degraded` to status `True`


OpenShift predefined condition types: https://github.com/openshift/api/blob/b1bcdbc3/config/v1/types_cluster_operator.go#L123-L140
Kubernetes Pod conditions: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions

QUESTION: should depending services be considered for status checks ? For example, should the Superset operator check that Druid services function properly ?

==== Pros

* Implementaion detail: reuse `ClusterResources` from the `operator-rs` framework as much as possible and have a generic way to update cluster status.
* Non-breaking CRD changes needed to add conditions.
* Transparent and easy to understand. Failure messages can be propagated from sub-resources hat have problems.


==== Cons

* It completely relies on Kubernetes resource status fields without querying the actual products. This assumes that liveliness probes and other checks htat Kubermetes performs, mirror the true product status.
* It requires that all resource dependencies run inside the Kubernetes cluster. For example, if Superset is configured with a Druid connection outside the Kubernetes cluster, the `Available` confition of the connection will have a status of `Unknown`.


[source,yaml]
----
status:
  conditions:
    - type: Available
      status: "True"
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
      message: "UI and Postgres DB running"
    - type: Degraded
      status: "True"
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
      reason: "DruidConnection failed. <Optional: Druid degraded message>"
    - type: Progressing
      status: "True"
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
      message: "New replicas starting."
    - type: Upgradable
      status: "Unknown"
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
    - type: Paused
      status: "True"
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
      message: "User requested reconcide pause."
----

Second example:

[source,yaml]
----
status:
  conditions:
    - type: Available
      status: "False"
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
      message: "No Pods running."
    - type: Stopped
      status: "True"
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
      reason: "User requested reconcile stop."
----

=== Set Status Custom Fields and Conditions

Most custom fields are set by querying the products directly. One exception is the deployed product version.

==== Pros

* Fine graned status information
* More reliable status information that is queried directly from the operated product and dependencies
* Products can run inside and outside the Kubernetes cluster

==== Cons

* Complexity and specificity of the implementation. Operators must implement product network protocols and metadata structures to be able to communicate with the products.
* Hard to maintain across product versions.
* Each new sub-resource requires additional code and dependencies.

==== Example CRD

[source,yaml]
----
apiVersion: zookeeper.stackable.tech/v1alpha1
kind: ZookeeperCluster
metadata:
  name: zookeeper
spec:
  image:
    productVersion: 3.8.0
    stackableVersion: "23.1"
  servers:
    config:
      podAffinity: # Whole struct is atomic. When you set something below this you are one your own
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - zookeeper
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                    - zookeeper
                  - key: app.kubernetes.io/component
                    operator: In
                    values:
                    - server
                  - key: app.kubernetes.io/role-group
                    operator: In
                    values:
                    - default
              topologyKey: "kubernetes.io/hostname"
        podAffinity: null
      nodeAffinity: # Whole struct is atomic. When you set something below this you are one your own
        nodeAffinity: null # We don't set any nodeAffinity as a default, but can be set from the user
        nodeSelector: null
    roleGroups:
      default:
        replicas: 3
        config:
          nodeAffinity:
            nodeSelector:
              machine: ultrafast # This will not overwrite the podAffinity setting, only the nodeAffinity
----


==== Pros

* Enables definining only one of the two structs an the CRD

==== Cons

* Creates a logical split between two entities that are closely related and should usually be kept together


=== Introduce one dedicated attribute

Same as Option "Introduce two dedicated attributes", but all the affinity related settings are below a attribute `affinity`.
Every setting is atomic for itself, so we can ship a pod anti-affinity in the defaults and a role can configure a pod affinity without overwriting our anti-affinity.

==== CRD

[source,yaml]
----
apiVersion: zookeeper.stackable.tech/v1alpha1
kind: ZookeeperCluster
metadata:
  name: zookeeper
spec:
  image:
    productVersion: 3.8.0
    stackableVersion: "23.1"
  servers:
    config:
      affinity:
        podAntiAffinity: # atomic
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - zookeeper
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                    - zookeeper
                  - key: app.kubernetes.io/component
                    operator: In
                    values:
                    - server
                  - key: app.kubernetes.io/role-group
                    operator: In
                    values:
                    - default
              topologyKey: "kubernetes.io/hostname"
        podAffinity: null # atomic
        nodeAffinity: null # atomic
        nodeSelector: null # atomic
    roleGroups:
      default:
        replicas: 3
        config:
          affinity:
            nodeSelector:
              machine: ultrafast # This will *only* overwrite the nodeSelector, nothing else
----

==== Pros

* Defines one common abstraction that can be reused everywhere and contains everything we might need

==== Cons

* Not able to use only one sort of affinity in CRDs

== Decision Outcome

Chosen option: "Introduce one dedicated attribute", because affinity is a feature we expect a sufficiently large number of customers to configure.
We don't want that these users to need to rely on `podOverwrite` for such as "basic feature".
This way we also express that we support configuring a different affinity officially.

=== Compatibility with existing `nodeSelector` field
We will keep, but deprecate, the existing `nodeSelector` field.
Existing CRDs with this field set will be treated by the operator as if the nodeSelector was set in the new struct, as defined by this ADR.
If both, `nodeSelector` at the top level and in the `affinities` field are defined the operator will throw an error and stop reconciliation.
This should not affect any pre-existing CR objects, as only one field exists at this time, so this will only affect changes after the implementation of this PR has gone live and the users should use the new functionality in this case.

== Default affinities per product
The default affinities should be as follows.
It should give a overall idea of what the affinities should look like, but does not claim to be a complete list.

*The List is sorted in ascending order of priority!*

*airflow:*

* Affinity between different roles
* Anti-affinity between all pods with the same role

*druid:*

* Affinity between different roles
* Affinity between different brokers and routers (the broker and router should ideally run on the same node (see https://druid.apache.org/docs/latest/design/processes.html[docs])
* Affinity of historicals to datanodes if hdfs is used for deep storage
* Anti-affinity between all pods with the same role

*hbase:*

* Affinity between different roles
* Affinity between regionservers and datanodes of the referenced HDFS
* Anti-affnity between all region servers
* Anti-affinity between all masters

*hdfs:*

* Affinity between different roles
* Anti-affinity between datanodes
* Anti-affinity between namenodes

*hive:*

* Anti-affinity between all HMS
* NOT RELEVANT: Affinity of HMS to datanodes if hdfs is used. TODO: Better to namenodes as we only do metadata operations? Is it even worth it, as we don't know which NN is active?

*kafka:*

* Anti-affinity between all kafka instances (We know this causes more replication traffic)

*nifi*

* Anti-affinity between all nifi instances

*opa*

* No affinity needed, because deployed as DaemonSet

*spark-k8s:*

* We currently don't support automatically connecting to HDFS clusters. If we start to do so: Affinity to datanodes
* Anti-affinity between all executors. Tradeoff is reliability <-> shuffle traffic. We choose reliability over traffic here, as someone makes such small executors that a node can handle multiple of them he is already asking for shuffle traffic.

*superset:*

* If DruidConnection is deployed affinity to routers
* We currently don't support TrinoConnection. If we start to do so: Affinity to coordinators
* Anti-affinity between all superset instances

*trino:*

* Anti-affinity between all worker. Tradeoff is reliability <-> exchange traffic. We choose reliability over traffic here, as someone makes such small executors that a node can handle multiple of them he is already asking for shuffle traffic.
* Anti-affinity between all coordinators. Currently only one coordinator is supported, but that might change in the future

*zookeeper:*

* Anti-affinity between all pods with the same role
