= Logging and Log Aggregation Architecture
Felix Hennig <felix.hennig@stackable.tech>
v0.1, 2022-10-26
:status: [draft]

* Status: {status}
* Deciders:
** Felix Hennig
** Lars Francke
** Malte Sander
** Razvan Mihai
** Sebastian Bernauer
** Siegfried Weber
** Sönke Liebau
** Teo Klestrup Röijezon
* Date: 2022-10-26

Technical Story: https://github.com/stackabletech/issues/issues/202, https://github.com/stackabletech/issues/issues/261

== Context and Problem Statement

// Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.

As a user of the platform, I have poor visibility into what is happening in different components of the platform, as well as how components influence each other, especially the operator and the product clusters it manages.

Logs usually give this insight. Currently, every pod logs to stdout with a certain default log format. For log configuration we support setting a custom log configuration file in some products.

Logs are **not persisted**, a crashed pod is difficult to investigate. Logs are **not aggregated**, investigating issues that involve multiple pods is difficult. Log **configuration is difficult** or impossible.

== Decision Drivers

* **identical configuration** - All product logs should be configurable in the same way, no matter their underlying logging framework (log4j, logback, Python logging, etc.). Every product should have a logging section in its CRD.
* **easily configured sinks** - A log sink (i.e. elasticsearch) should only be configured in a single place for the whole platform (not in every product instance).
* **Support plaintext logs on stdout** - It is a Kubernetes convention to log in plaintext to stdout. This should be kept, as it is a useful tool for interactive debugging.
* **custom overrides** - The log format, aggregator and sink should be substitutable by the user.
* **transport security** - Logs should transmitted with encryption/TLS.

=== Assumptions

* **RegEx parsing is error prone** - Parsing pure string log output to get structure (time, module, log level, message, ...) is prone to errors. Especially for multi-line messages like stacktraces. The design should use structured log ouput whenever possible.

=== Constraints

* **Multiple files** - Some products like HDFS _need_ to log to (multiple) files, so parsing just stdout is not sufficient; we need to parse files.

== Decision

We decided to use https://vector.dev/[Vector] as an aggregator. It looked good, no other options were evaluated.

We chose to provide **plaintext logs on stdout** and **structured logs on file**. For the architecture, we chose to deploy the agent as a **sidecar** and **use an aggregator**. We chose to provide **OpenSearch** as a default Sink for logs.

Find detailed considerations below.

=== Log Aggregation Framework: Vector

https://vector.dev/[Vector] has been picked without comparison to other existing choices. Some googling shows https://medium.com/ibm-cloud/log-collectors-performance-benchmarking-8c5218a08fea[Vector has better performance than fluentd], the biggest name in this space. Vector is also open source and written in Rust: https://github.com/vectordotdev/vector[GitHub].

=== Log Retrieval Strategy

We want to parse logs out of structured log entries and also keep the stdout log as plaintext. Also, some products require reading log files instead of stdout. Which means **we opt to read logs from files for every product** and **parse structured log entries**.

[#log_aggregation_architecture]
=== Log Aggregation Architecture

**Agent role** - Vector offers multiple https://vector.dev/docs/setup/deployment/roles/#agent[roles] in which it can run. The DaemonSet is easier to deploy, and only one vector instance is running per host. But the sidecar has deeper integration with the product container and can read log files, something that we decided we need.

**Topology** - Vector offers multiple https://vector.dev/docs/setup/deployment/topologies/[topologies] and already lists pros and cons. An aggregator makes it easier to configure a sink: only one place to configure it (the aggregator has "multi-host context" as Vector calls it). It also reduces requests made to the sink, as it aggregates and buffers requests.

The connection from the agent to the aggregator is made with the help of a **discovery ConfigMap**. The aggregator is discoverable with a discovery ConfigMap, which is referenced in the ProductCluster with a property called `vectorAggregatorConfigMapName`, similar to `zookeeperConfigMapName` or `hdfsConfigMapName`. The property is located at the top level of the spec.

=== Log Sink: OpenSearch

https://opensearch.org/[OpenSearch] has been picked as the sink for all the aggregated logs. No other options have been considered, but as the sink sits at the very top of the chain, it is easy to swap it out for a different software.

The sink is configured in the Vector aggregator. We initially do not provide any "Stackable way" of configuring the sink, but it is easily configured by the user in a single location for the whole platform. Other sinks can be configured easily as well (https://vector.dev/docs/reference/configuration/sinks/[Vector docs for sink configuration]). See also <<deploying_the_stack>>.

**For now, setting up the aggregator and sink is the responsibility of the user**.

=== Logging configuration

**Log levels** - The following log levels are supported: ERROR, WARN, INFO, DEBUG, and TRACE. We support them across all products. The **default log level** is INFO. Products might have slightly different log levels, which are mapped accordingly by the agent. For instance, Superset emits logs with the levels CRITICAL, ERROR, WARNING, INFO, and DEBUG. The mapping should be:

[cols="1,1"]
|===
|Superset log level | Stackable log level 

|CRITICAL
| ERROR

| ERROR
| ERROR

| WARNING
| WARN

| INFO
| INFO

| DEBUG
| DEBUG

| DEBUG
| TRACE
|===

There is no TRACE log level in Superset, so if the user sets the desired log level to TRACE then it is actually set to DEBUG in Superset.

**console vs. file** - We want to have different log levels (and possibly other settings) for console (stdout) and file output. This makes debugging easier, without also filling up the log aggregator with very chatty logs.

**Settings per logger** - Most products (especially Java based ones) support setting a log level per package. We also want to support this in our CRD.

```
logging:
  file:
    loggers:
      ROOT:
        level: INFO
      some.interesting.module:
        level: DEBUG
  console:
    loggers:
      ROOT:
        level: DEBUG
      my.specific.package:
        level: TRACE
      my.other.package:
        level: TRACE
```

**Settings per role** - All of these should be configurable per role and role group. Some sub-loggers are only available in certain roles.

```
spec:
  someRole:
    config:
      logging:
        ...
    roleGroups:
      default:
        logging:
          ...
      aDifferentGroup:
        logging:
          ...
```

**Override everything** - The customer should be able to supply their own configuration file. Where this is placed depends on the product.

```
logging:
  custom:
    configMap: nameOfMyConfigMapWithTheConfigFile
```

Like the other logging settings, this costum configuration file can be supplied per role and/or role-group.

Setting the `custom` field will disable any configurations made in `file` and `console`. (TODO maybe we can disallow this alltogether in the CRD type)

**Disable vector** - Vector should be optional, if the user wants to use their own logging system.

```
logging:
  enableVectorAgent: false  # defaults to true
```

[#deploying_the_stack]
=== Deploying the Stack

The operator deploys the Vector agent as a sidecar and deploys the logging configuration for the product.

The aggregator and OpenSearch sink are deployed with a stackablectl Stack for now. The Stack also supports deploying the <<log_aggregation_architecture, aggregator ConfigMap>>. A more integrated way of deployment and configuration of the aggregator and sink is still to be defined, see <<future_work>>.

== Consequences


=== Positive

Logs across the platform (from products and operators) are **persisted** and **aggregated** in a central location. Crashed pods can be investigated, as well as issues involving multiple products.

=== Negative

* Every pod will contain a vector sidecar container, which adds overhead.
* The unified logging configuration hides product specific logging settings.

Changing a log level might lead to a pod getting restarted.

[#future_work]
== Future work that will become necessary

We will have to better integrate the deployment of the Vector aggregator and the OpenSearch sink into Stackable.

== Links

* https://vector.dev/[Vector]
* https://vector.dev/docs/setup/deployment/roles/[Vector Deployment Roles]
* https://vector.dev/docs/setup/deployment/topologies/[Vector Deployment Topologies]
